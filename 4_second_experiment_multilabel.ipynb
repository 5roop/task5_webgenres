{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import parse\n",
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "datadir = \"/home/peterr/macocu/task5_webgenres/data/final/fasttext1\"\n",
    "\n",
    "dev_subset = os.path.join(datadir, \"dev_onlykeep_True_onlyprimary_False.fasttext\")\n",
    "test_subset = os.path.join(datadir, \"test_onlykeep_True_onlyprimary_False.fasttext\")\n",
    "train_subset = os.path.join(datadir, \"train_onlykeep_True_onlyprimary_False.fasttext\")\n",
    "\n",
    "dev_full = os.path.join(datadir, \"dev_onlykeep_False_onlyprimary_False.fasttext\")\n",
    "test_full = os.path.join(datadir, \"test_onlykeep_False_onlyprimary_False.fasttext\")\n",
    "train_full = os.path.join(datadir, \"train_onlykeep_False_onlyprimary_False.fasttext\")\n",
    "\n",
    "def parse_test_file(path: str):\n",
    "    \"\"\"Reads fasttext formatted file and returns labels, texts.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    pattern = \"{label} {text}\\n\"\n",
    "    p = parse.compile(pattern)\n",
    "\n",
    "    labels, texts = list(), list()\n",
    "    for line in content:\n",
    "        rez = p.parse(line)\n",
    "        labels.append(rez[\"label\"])\n",
    "        texts.append(rez[\"text\"])\n",
    "    return labels, texts\n",
    "\n",
    "def prediction_to_label(prediction):\n",
    "    \"\"\"Transforms predicitons as returned by fasttext into pure labels.\"\"\"\n",
    "    import numpy as np\n",
    "    return np.array(prediction[0])[:, 0]\n",
    "\n",
    "all_labels = [\n",
    " '__label__Promotion_of_Services',\n",
    " '__label__Instruction',\n",
    " '__label__Review',\n",
    " '__label__Information/Explanation',\n",
    " '__label__Promotion_of_a_Product',\n",
    " '__label__News/Reporting',\n",
    " '__label__Promotion',\n",
    " '__label__Announcement',\n",
    " '__label__Invitation',\n",
    " '__label__Opinion/Argumentation',\n",
    " '__label__Forum',\n",
    " '__label__Legal/Regulation',\n",
    " '__label__Other',\n",
    " '__label__Opinionated_News',\n",
    " '__label__Call',\n",
    " '__label__List_of_Summaries/Excerpts']\n",
    "\n",
    "def get_true_labels(path):\n",
    "    \"\"\"Reads the test file and returns labeldict, texts\"\"\"\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    pattern = \"__label__{label} {text}\\n\"\n",
    "    p = parse.compile(pattern)\n",
    "    labels = list()\n",
    "    texts = list()\n",
    "    for l1, l2, l3 in zip(content[::3], content[1::3], content[2::3]):\n",
    "        distribution = dict()\n",
    "        r1 = p.parse(l1)\n",
    "        r2 = p.parse(l2)\n",
    "        r3 = p.parse(l3)\n",
    "        assert r1 is not None, f\"Parsing raised a None: {r1=}\"\n",
    "        assert r2 is not None, f\"Parsing raised a None: {r2=}\"\n",
    "        assert r3 is not None, f\"Parsing raised a None: {r3=}\"\n",
    "        assert r1[\"label\"] == r2[\"label\"], f\"Primary label mismatch! \\n{r1=}\\n{r2=}\\n{r3=}\"\n",
    "        assert r1[\"text\"] == r2[\"text\"]  == r3[\"text\"], f\"Text mismatch! \\n{r1=}\\n{r2=}\\n{r3=}\"\n",
    "        texts.append(r2[\"text\"])\n",
    "        primary_label = \"__label__\" + r1[\"label\"]\n",
    "        secondary_label = \"__label__\" + r3[\"label\"]\n",
    "\n",
    "        distribution[primary_label] = 2/3\n",
    "        distribution[secondary_label] = distribution.get(secondary_label, 0) + 1/3\n",
    "\n",
    "        assert sum([i for i in distribution.values()]) == 1, f\"Distribution does not add to 1!\\n{distribution=}\"\n",
    "\n",
    "        labels.append(distribution)\n",
    "\n",
    "    return labels, texts\n",
    "\n",
    "def get_predicted_labels(model, texts):\n",
    "\n",
    "    \"\"\"Uses the model to predict the labels of the texts.\n",
    "    \n",
    "    Returns a list of dictionaries with label distributions.\"\"\"\n",
    "\n",
    "    predictions = model.predict(texts, k=-1)\n",
    "    y_pred = list()\n",
    "    for labels, probabilities in zip(*predictions):\n",
    "        distribution = dict()\n",
    "        for label, probability in zip(labels, probabilities):\n",
    "            distribution[label] = probability\n",
    "        y_pred.append(distribution)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = fasttext.train_supervised(input=train_full, autotuneValidationFile=dev_full, autotuneDuration=1200)\n",
    "model_subset = fasttext.train_supervised(input=train_subset, autotuneValidationFile=dev_subset, autotuneDuration=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_full, texts_full = get_true_labels(test_full)\n",
    "y_true_subset, texts_subset = get_true_labels(test_subset)\n",
    "\n",
    "y_pred_full = get_predicted_labels(model_full, texts_full)\n",
    "y_pred_subset = get_predicted_labels(model_subset, texts_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__Review': 0.6666666666666666,\n",
       " '__label__Information/Explanation': 0.3333333333333333}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_full[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__Opinion/Argumentation': 0.20486838,\n",
       " '__label__Information/Explanation': 0.1521792,\n",
       " '__label__List_of_Summaries/Excerpts': 0.14388034,\n",
       " '__label__Opinionated_News': 0.13404168,\n",
       " '__label__News/Reporting': 0.08886625,\n",
       " '__label__Other': 0.03339771,\n",
       " '__label__Forum': 0.032761667,\n",
       " '__label__Promotion': 0.031631626,\n",
       " '__label__Promotion_of_a_Product': 0.029673748,\n",
       " '__label__Review': 0.024865033,\n",
       " '__label__Instruction': 0.020232134,\n",
       " '__label__Promotion_of_Services': 0.01774978,\n",
       " '__label__Correspondence': 0.016739119,\n",
       " '__label__Legal/Regulation': 0.01468199,\n",
       " '__label__Invitation': 0.012260729,\n",
       " '__label__Research_Article': 0.006893041,\n",
       " '__label__Call': 0.006548532,\n",
       " '__label__Announcement': 0.006082948,\n",
       " '__label__Interview': 0.005877393,\n",
       " '__label__Prose': 0.0058747143,\n",
       " '__label__Promotion_of_services': 0.0029758418,\n",
       " '__label__Opinionated_news': 0.0029177745,\n",
       " '__label__Recipe': 0.002812528,\n",
       " '__label__Promotion_of_a_product': 0.0024278548}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_full[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binarize_distribution(distribution, cutoff = 0.1):\n",
    "    \"\"\"Sorts the probabilities distribution dict in a fixed order.\n",
    "    Compares the probabilities to the cutoff and returns 1 where they are bigger\n",
    "    than cutoff and 0 otherwise. Returns a list of binary values.\"\"\"\n",
    "\n",
    "    probabilities = [distribution.get(label, 0) for label in all_labels]\n",
    "\n",
    "    binarized_probabilities = [1 if i >= cutoff else 0 for i in probabilities ]\n",
    "    \n",
    "    return binarized_probabilities\n",
    "\n",
    "binarize_distribution(y_true_full[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_full_binary = [binarize_distribution(i) for i in y_true_full]\n",
    "y_true_subset_binary = [binarize_distribution(i) for i in y_true_subset]\n",
    "y_pred_full_binary = [binarize_distribution(i) for i in y_pred_full]\n",
    "y_pred_subset_binary = [binarize_distribution(i) for i in y_pred_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22909610659019153"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true_full_binary, y_pred_full_binary, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
