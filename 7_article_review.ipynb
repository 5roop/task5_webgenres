{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to reproduce the results of _Beyond the English web: ..._\n",
    "\n",
    "Finnish monolingual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [\"HI\"] * (62+27+5+6) +\\\n",
    "        [\"ID\"] * (55+13+13+9+9) +\\\n",
    "        [\"IN\"] * (7+2+60+2+14+9+6) +\\\n",
    "        [\"IP\"] * (2 +13+67+7+8+3) +\\\n",
    "        [\"NA\"] * (1+3+1+86+5+3)+\\\n",
    "        [\"OP\"] * (1+2+6+2+13+71+5)\n",
    "y_pred = [\"HI\"]*62 + [\"IN\"] * 27 + [\"IP\"] * 5 + [\"HYB\"] * 6+\\\n",
    "        [\"ID\"] * 55 + [\"IN\"] * 13 + [\"NA\"] * 13 + [\"OP\"] * 9 +[\"HYB\"] *9+\\\n",
    "        [\"HI\"] * 7+[\"ID\"] *2 +[\"IN\"] *60 +[\"IP\"] * 2 +[\"NA\"] *14 +[\"OP\"] *9 +[\"HYB\"] * 6 +\\\n",
    "        [\"HI\"] * 2 +[\"IN\"] *13 +[\"IP\"] * 67 +[\"NA\"] * 7 +[\"OP\"] *8+[\"HYB\"] * 3 +\\\n",
    "        [\"HI\"] * 1 +[\"IN\"] * 3 + [\"IP\"] +[\"NA\"] * 86+[\"OP\"] *5+[\"HYB\"] *3+\\\n",
    "        [\"HI\"] * 1 +[\"ID\"] * 2 +[\"IN\"] * 6 +[\"IP\"] * 2 +[\"NA\"] * 13 +[\"OP\"] * 71 +[\"HYB\"] *5\n",
    "labels = \"HI,ID,IN,IP,NA,OP,HYB\".split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.71676301 0.69620253 0.54054054 0.75706215 0.74137931 0.7029703\n",
      " 0.        ]\n",
      "average='micro' F1 score: 0.6705685618729097\n",
      "average='macro' F1 score: 0.5935596903190917\n",
      "average='weighted' F1 score: 0.692398330069187\n",
      "[[62  0 27  5  0  0  6]\n",
      " [ 0 55 13  0 13  9  9]\n",
      " [ 7  2 60  2 14  9  6]\n",
      " [ 2  0 13 67  7  8  3]\n",
      " [ 1  0  3  1 86  5  3]\n",
      " [ 1  2  6  2 13 71  5]\n",
      " [ 0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swedish monolingual:\n",
    "\n",
    "We expect either 0.8261 or 0.8304:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.62251656 0.6918239  0.79111111 0.76635514 0.67953668 0.71351351\n",
      " 0.31578947]\n",
      "average='micro' F1 score: 0.6562054208273894\n",
      "average='macro' F1 score: 0.6543780533849844\n",
      "average='weighted' F1 score: 0.6545377924531753\n",
      "[[47  0 15  0  0  0 38]\n",
      " [ 0 55  7  0 27  8  3]\n",
      " [ 0  1 89  3  2  0  5]\n",
      " [ 0  0  4 82  3  3  9]\n",
      " [ 0  1  2  1 88  1  7]\n",
      " [ 0  1  0  7 12 66 14]\n",
      " [ 4  1  8 20 27  7 33]]\n"
     ]
    }
   ],
   "source": [
    "y_true = [\"HI\"] * (47+15+38) +\\\n",
    "        [\"ID\"] * (55+7+27+8+3) +\\\n",
    "        [\"IN\"] * (1+89+3+2+5) +\\\n",
    "        [\"IP\"] * (4+82+3+3+9) +\\\n",
    "        [\"NA\"] * (1+2+1+88+1+7)+\\\n",
    "        [\"OP\"] * (1+7+12+66+14)+\\\n",
    "        [\"HYB\"] * (4+1+8+20+27+7+33)\n",
    "\n",
    "HI = [\"HI\"]\n",
    "ID = [\"ID\"]\n",
    "IN = [\"IN\"]\n",
    "IP= [\"IP\"]\n",
    "NA= [\"NA\"]\n",
    "OP= [\"OP\"]\n",
    "HYB= [\"HYB\"]\n",
    "\n",
    "\n",
    "y_pred = HI * 47 +IN *15 + HYB * 38 +\\\n",
    "    ID * 55 + IN *7 + NA* 27 +OP *8 + HYB * 3 +\\\n",
    "        ID +IN * 89 + IP*3+NA*2+HYB*5+\\\n",
    "    IN * 4 + IP *82 + NA * 3 + OP * 3 + HYB * 9+\\\n",
    "        ID + IN * 2 + IP + NA * 88 + OP + HYB *7+\\\n",
    "    ID + IP * 7 + NA * 12 + OP * 66 + HYB * 14 + \\\n",
    "        HI * 4 + ID * 1 + IN * 8 + IP * 20 + NA * 27 + OP * 7 + HYB*33\n",
    "labels = \"HI,ID,IN,IP,NA,OP,HYB\".split(\",\")\n",
    "\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article: Multilingual and Zero-shot is closing in on monolingual web register classification\n",
    "\n",
    "Figure 3 confusion matrix will be replicated and F1 scores will be calculated on that data.\n",
    "\n",
    "English master model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.66666667 0.88297872 0.55598456 0.41860465 0.6484375  0.57264957\n",
      " 0.15819209]\n",
      "average='micro' F1 score: 0.5687679083094556\n",
      "average='macro' F1 score: 0.5576448228947603\n",
      "average='weighted' F1 score: 0.5575292587361327\n",
      "[[51  1 10  0  2  7 27]\n",
      " [ 0 83  5  0  5  6  1]\n",
      " [ 1  1 72  1 10  8  8]\n",
      " [ 0  0 36 27  8 16 12]\n",
      " [ 0  1  5  0 83  6  5]\n",
      " [ 1  1  7  0 14 67 10]\n",
      " [ 2  1 23  2 34 24 14]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_cm = np.array([\n",
    "[51,1,10,0,2,7,27],\n",
    "[0,83,5,0,5,6,1],\n",
    "[1,1,72,1,10,8,8],\n",
    "[0,0,36,27,8,16,12],\n",
    "[0,1,5,0,83,6,5],\n",
    "[1,1,7,0,14,67,10],\n",
    "[2,1,23,2,34,24,14]\n",
    "],)\n",
    "\n",
    "\n",
    "labels = \"HI,ID,IN,IP,NA,OP,HYB\".split(\",\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for label, num in zip(labels, input_cm.sum(axis=1)):\n",
    "    y_true.extend([label] * num)\n",
    "for row in input_cm:\n",
    "    for label, num in zip(labels, row):\n",
    "        y_pred.extend([label] * num)\n",
    "\n",
    "\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre Annotation for the Web:\n",
    "english results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.73459716 0.8195122  0.78037383 0.65591398 0.5248227  0.74226804\n",
      " 0.89842632 0.73646209 0.7254902  0.78888889]\n",
      "average='micro' F1 score: 0.7605276509386099\n",
      "average='macro' F1 score: 0.7406755400212118\n",
      "average='weighted' F1 score: 0.7575848894023204\n",
      "[[310   2   2  16   2  12  10   6   3   2]\n",
      " [  7  84   1   0   0   6   1   1   1   1]\n",
      " [ 12   4 167   1   2  10   7  12   3   2]\n",
      " [ 53   0   1 122   1   8   7   0   7   2]\n",
      " [ 18   0  14   6  37   1   2  13   2   0]\n",
      " [ 22   4   6   2   0 144   3   1   2   0]\n",
      " [  7   0   5   8   1   6 314   2   3   2]\n",
      " [  9   1   2   2   0   5   0 102   4   0]\n",
      " [ 35   6   5  11   5   2   7  14 148   1]\n",
      " [  6   2   5   3   0  10   0   1   1  71]]\n",
      "average=None Precision: [0.64718163 0.81553398 0.80288462 0.71345029 0.77083333 0.70588235\n",
      " 0.89458689 0.67105263 0.85057471 0.87654321]\n",
      "average='micro' Precision: 0.7605276509386099\n",
      "average='macro' Precision: 0.7748523651717859\n",
      "average='weighted' Precision: 0.7722089377373934\n",
      "average=None Recall: [0.84931507 0.82352941 0.75909091 0.60696517 0.39784946 0.7826087\n",
      " 0.90229885 0.816      0.63247863 0.71717172]\n",
      "average='micro' Recall: 0.7605276509386099\n",
      "average='macro' Recall: 0.7287307921720946\n",
      "average='weighted' Recall: 0.7605276509386099\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "input_cm = np.array([\n",
    "[310,2,2,16,2,12,10,6,3,2],\n",
    "[7,84,1,0,0,6,1,1,1,1],\n",
    "[12,4,167,1,2,10,7,12,3,2],\n",
    "[53,0,1,122,1,8,7,0,7,2],\n",
    "[18,0,14,6,37,1,2,13,2,0],\n",
    "[22,4,6,2,0,144,3,1,2,0],\n",
    "[7,0,5,8,1,6,314,2,3,2],\n",
    "[9,1,2,2,0,5,0,102,4,0],\n",
    "[35,6,5,11,5,2,7,14,148,1],\n",
    "[6,2,5,3,0,10,0,1,1,71]\n",
    "],)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels = \"A1,A4,A7,A8,A9,A11,A12,A14,A16,A17\".split(\",\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for label, num in zip(labels, input_cm.sum(axis=1)):\n",
    "    y_true.extend([label] * num)\n",
    "for row in input_cm:\n",
    "    for label, num in zip(labels, row):\n",
    "        y_pred.extend([label] * num)\n",
    "\n",
    "\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred, labels=labels, average=average)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87204234, 0.87451644, 0.93975078, 0.8944572 , 0.80875515,\n",
       "       0.85591492, 0.90668177, 0.86783786, 0.78964078, 0.69599608])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "\n",
    "lb.fit(y_true)\n",
    "y_true_bin = lb.transform(y_true)\n",
    "y_pred_bin = lb.transform(y_pred)\n",
    "roc_auc_score(y_true_bin, y_pred_bin, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
