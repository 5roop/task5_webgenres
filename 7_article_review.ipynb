{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to reproduce the results of _Beyond the English web: ..._\n",
    "\n",
    "Finnish monolingual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [\"HI\"] * (62+27+5+6) +\\\n",
    "        [\"ID\"] * (55+13+13+9+9) +\\\n",
    "        [\"IN\"] * (7+2+60+2+14+9+6) +\\\n",
    "        [\"IP\"] * (2 +13+67+7+8+3) +\\\n",
    "        [\"NA\"] * (1+3+1+86+5+3)+\\\n",
    "        [\"OP\"] * (1+2+6+2+13+71+5)\n",
    "y_pred = [\"HI\"]*62 + [\"IN\"] * 27 + [\"IP\"] * 5 + [\"HYB\"] * 6+\\\n",
    "        [\"ID\"] * 55 + [\"IN\"] * 13 + [\"NA\"] * 13 + [\"OP\"] * 9 +[\"HYB\"] *9+\\\n",
    "        [\"HI\"] * 7+[\"ID\"] *2 +[\"IN\"] *60 +[\"IP\"] * 2 +[\"NA\"] *14 +[\"OP\"] *9 +[\"HYB\"] * 6 +\\\n",
    "        [\"HI\"] * 2 +[\"IN\"] *13 +[\"IP\"] * 67 +[\"NA\"] * 7 +[\"OP\"] *8+[\"HYB\"] * 3 +\\\n",
    "        [\"HI\"] * 1 +[\"IN\"] * 3 + [\"IP\"] +[\"NA\"] * 86+[\"OP\"] *5+[\"HYB\"] *3+\\\n",
    "        [\"HI\"] * 1 +[\"ID\"] * 2 +[\"IN\"] * 6 +[\"IP\"] * 2 +[\"NA\"] * 13 +[\"OP\"] * 71 +[\"HYB\"] *5\n",
    "labels = \"HI,ID,IN,IP,NA,OP,HYB\".split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.71676301 0.69620253 0.54054054 0.75706215 0.74137931 0.7029703\n",
      " 0.        ]\n",
      "average='micro' F1 score: 0.6705685618729097\n",
      "average='macro' F1 score: 0.5935596903190917\n",
      "average='weighted' F1 score: 0.692398330069187\n",
      "[[62  0 27  5  0  0  6]\n",
      " [ 0 55 13  0 13  9  9]\n",
      " [ 7  2 60  2 14  9  6]\n",
      " [ 2  0 13 67  7  8  3]\n",
      " [ 1  0  3  1 86  5  3]\n",
      " [ 1  2  6  2 13 71  5]\n",
      " [ 0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swedish monolingual:\n",
    "\n",
    "We expect either 0.8261 or 0.8304:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.62251656 0.6918239  0.79111111 0.76635514 0.67953668 0.71351351\n",
      " 0.31578947]\n",
      "average='micro' F1 score: 0.6562054208273894\n",
      "average='macro' F1 score: 0.6543780533849844\n",
      "average='weighted' F1 score: 0.6545377924531753\n",
      "[[47  0 15  0  0  0 38]\n",
      " [ 0 55  7  0 27  8  3]\n",
      " [ 0  1 89  3  2  0  5]\n",
      " [ 0  0  4 82  3  3  9]\n",
      " [ 0  1  2  1 88  1  7]\n",
      " [ 0  1  0  7 12 66 14]\n",
      " [ 4  1  8 20 27  7 33]]\n"
     ]
    }
   ],
   "source": [
    "y_true = [\"HI\"] * (47+15+38) +\\\n",
    "        [\"ID\"] * (55+7+27+8+3) +\\\n",
    "        [\"IN\"] * (1+89+3+2+5) +\\\n",
    "        [\"IP\"] * (4+82+3+3+9) +\\\n",
    "        [\"NA\"] * (1+2+1+88+1+7)+\\\n",
    "        [\"OP\"] * (1+7+12+66+14)+\\\n",
    "        [\"HYB\"] * (4+1+8+20+27+7+33)\n",
    "\n",
    "HI = [\"HI\"]\n",
    "ID = [\"ID\"]\n",
    "IN = [\"IN\"]\n",
    "IP= [\"IP\"]\n",
    "NA= [\"NA\"]\n",
    "OP= [\"OP\"]\n",
    "HYB= [\"HYB\"]\n",
    "\n",
    "\n",
    "y_pred = HI * 47 +IN *15 + HYB * 38 +\\\n",
    "    ID * 55 + IN *7 + NA* 27 +OP *8 + HYB * 3 +\\\n",
    "        ID +IN * 89 + IP*3+NA*2+HYB*5+\\\n",
    "    IN * 4 + IP *82 + NA * 3 + OP * 3 + HYB * 9+\\\n",
    "        ID + IN * 2 + IP + NA * 88 + OP + HYB *7+\\\n",
    "    ID + IP * 7 + NA * 12 + OP * 66 + HYB * 14 + \\\n",
    "        HI * 4 + ID * 1 + IN * 8 + IP * 20 + NA * 27 + OP * 7 + HYB*33\n",
    "labels = \"HI,ID,IN,IP,NA,OP,HYB\".split(\",\")\n",
    "\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article: Multilingual and Zero-shot is closing in on monolingual web register classification\n",
    "\n",
    "Figure 3 confusion matrix will be replicated and F1 scores will be calculated on that data.\n",
    "\n",
    "English master model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average=None F1 score: [0.66666667 0.88297872 0.55598456 0.41860465 0.6484375  0.57264957\n",
      " 0.15819209]\n",
      "average='micro' F1 score: 0.5687679083094556\n",
      "average='macro' F1 score: 0.5576448228947603\n",
      "average='weighted' F1 score: 0.5575292587361327\n",
      "[[51  1 10  0  2  7 27]\n",
      " [ 0 83  5  0  5  6  1]\n",
      " [ 1  1 72  1 10  8  8]\n",
      " [ 0  0 36 27  8 16 12]\n",
      " [ 0  1  5  0 83  6  5]\n",
      " [ 1  1  7  0 14 67 10]\n",
      " [ 2  1 23  2 34 24 14]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_cm = np.array([\n",
    "[51,1,10,0,2,7,27],\n",
    "[0,83,5,0,5,6,1],\n",
    "[1,1,72,1,10,8,8],\n",
    "[0,0,36,27,8,16,12],\n",
    "[0,1,5,0,83,6,5],\n",
    "[1,1,7,0,14,67,10],\n",
    "[2,1,23,2,34,24,14]\n",
    "],)\n",
    "\n",
    "\n",
    "labels = \"HI,ID,IN,IP,NA,OP,HYB\".split(\",\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for label, num in zip(labels, input_cm.sum(axis=1)):\n",
    "    y_true.extend([label] * num)\n",
    "for row in input_cm:\n",
    "    for label, num in zip(labels, row):\n",
    "        y_pred.extend([label] * num)\n",
    "\n",
    "\n",
    "for average in [None, \"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"{average=}\", end =\" \")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred, labels=labels, average=average)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, )\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:73: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3c819711b02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ovo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0m\u001b[1;32m    386\u001b[0m                                          multi_class, average, sample_weight)\n\u001b[1;32m    387\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n\u001b[1;32m    442\u001b[0m     \u001b[0;31m# validation of the input y_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         raise ValueError(\n\u001b[1;32m    445\u001b[0m             \u001b[0;34m\"Target scores need to be probabilities for multiclass \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred, labels=labels, multi_class = \"ovo\")\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
