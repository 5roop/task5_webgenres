{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, all models will be trained and saved. In another notebook they will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:489: UserWarning: use_multiprocessing automatically disabled as camembert fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:941: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'microF1': 0.585, 'macroF1': 0.571702071141847, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.575, 'macroF1': 0.5038823287921238, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.58, 'macroF1': 0.5404399893933838, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.585, 'macroF1': 0.5966322527043888, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.59, 'macroF1': 0.5424094544007411, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.5875, 'macroF1': 0.5722383837732403, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.625, 'macroF1': 0.622224565151736, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.555, 'macroF1': 0.5010989686144965, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.59, 'macroF1': 0.5567143173798803, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.6, 'macroF1': 0.5703267983378637, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.56, 'macroF1': 0.42015898713758115, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.58, 'macroF1': 0.5053397073132861, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.605, 'macroF1': 0.632368854106175, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.575, 'macroF1': 0.47780399931031653, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.59, 'macroF1': 0.580195315365902, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.5879396984924623, 'macroF1': 0.5038281163281163, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.649746192893401, 'macroF1': 0.595564630743915, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6186868686868687, 'macroF1': 0.5803406826973889, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.5829145728643216, 'macroF1': 0.5207253296225728, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6040609137055838, 'macroF1': 0.5759216626719054, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.5934343434343434, 'macroF1': 0.5680276087477809, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.6130653266331658, 'macroF1': 0.5546765995553058, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6446700507614214, 'macroF1': 0.6244586400487021, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6287878787878788, 'macroF1': 0.6099193610826058, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.6080402010050251, 'macroF1': 0.562660449245815, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6345177664974619, 'macroF1': 0.5933186320037779, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6212121212121212, 'macroF1': 0.5904571590548051, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.6030150753768844, 'macroF1': 0.5716186201053716, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6345177664974619, 'macroF1': 0.6478570313974041, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6186868686868687, 'macroF1': 0.6238302864618946, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.628140703517588, 'macroF1': 0.5517553731491014, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.5736040609137056, 'macroF1': 0.5375974918247926, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.601010101010101, 'macroF1': 0.5875220073937228, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.555, 'macroF1': 0.57934274719989, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.54, 'macroF1': 0.4589341610770183, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.5475, 'macroF1': 0.5499018167455739, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.6180904522613065, 'macroF1': 0.5902982296701889, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.583756345177665, 'macroF1': 0.5096477585435083, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.601010101010101, 'macroF1': 0.576492004981009, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.52, 'macroF1': 0.4530735897305683, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.545, 'macroF1': 0.5147501934458456, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.5325, 'macroF1': 0.5071389468406606, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.6030150753768844, 'macroF1': 0.5045843077140069, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.6142131979695431, 'macroF1': 0.5535002035002036, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.6085858585858586, 'macroF1': 0.5527047584578121, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.505, 'macroF1': 0.4668973561223078, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.515, 'macroF1': 0.37032360670609077, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.51, 'macroF1': 0.44532519958269345, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.5879396984924623, 'macroF1': 0.5431382061183072, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.6142131979695431, 'macroF1': 0.5419233676376534, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.601010101010101, 'macroF1': 0.5814259630191666, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.565, 'macroF1': 0.5492759431653833, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.52, 'macroF1': 0.44482366817028324, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.5425, 'macroF1': 0.5204857435722245, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.6030150753768844, 'macroF1': 0.5288481853331477, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.5939086294416244, 'macroF1': 0.5657871725845822, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.5984848484848485, 'macroF1': 0.5797093299185282, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.555, 'macroF1': 0.5836125025162565, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.545, 'macroF1': 0.44747874696681805, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.55, 'macroF1': 0.5410152446123593, 'train': 'ok', 'eval': 'devtest_full'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import parse\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "datadir = \"/home/peterr/macocu/task5_webgenres/data/final/fasttext2\"\n",
    "\n",
    "dev_full = os.path.join(datadir, \"dev_onlyprimary_True_dedup_False.fasttext\")\n",
    "test_full = os.path.join(datadir, \"test_onlyprimary_True_dedup_False.fasttext\")\n",
    "train_full = os.path.join(datadir, \"train_onlyprimary_True_dedup_False.fasttext\")\n",
    "\n",
    "\n",
    "dev_dd = os.path.join(datadir, \"dev_onlyprimary_True_dedup_True.fasttext\")\n",
    "test_dd = os.path.join(datadir, \"test_onlyprimary_True_dedup_True.fasttext\")\n",
    "train_dd = os.path.join(datadir, \"train_onlyprimary_True_dedup_True.fasttext\")\n",
    "\n",
    "# dev_ok = os.path.join(datadir, \"dev_onlyprimary_True_dedup_True.fasttext\")\n",
    "# test_ok = os.path.join(datadir, \"test_onlyprimary_True_dedup_True.fasttext\")\n",
    "train_ok = os.path.join(datadir, \"train_onlyprimary_True_only_keep_True.fasttext\")\n",
    "\n",
    "\n",
    "train_labels = ['__label__Legal/Regulation', '__label__Opinionated_News', '__label__News/Reporting', '__label__Forum', '__label__Correspondence', '__label__Invitation', '__label__Instruction', '__label__Recipe', '__label__Opinion/Argumentation', '__label__Promotion_of_Services', '__label__Promotion', '__label__List_of_Summaries/Excerpts', '__label__Promotion_of_a_Product', '__label__Call', '__label__Review', '__label__Other', '__label__Information/Explanation', '__label__Interview', '__label__Prose', '__label__Research_Article', '__label__Announcement']\n",
    "\n",
    "STR_TO_NUM = {s: i for i, s in enumerate(train_labels)}\n",
    "NUM_TO_STR = {i: s for i, s in enumerate(train_labels)}\n",
    "\n",
    "\n",
    "def parse_fasttext_file(path: str, encode=True):\n",
    "    \"\"\"Reads fasttext formatted file and returns dataframe.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    pattern = \"{label} {text}\\n\"\n",
    "    p = parse.compile(pattern)\n",
    "\n",
    "    labels, texts = list(), list()\n",
    "    for line in content:\n",
    "        rez = p.parse(line)\n",
    "        if rez is not None:\n",
    "            labels.append(rez[\"label\"])\n",
    "            texts.append(rez[\"text\"])\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"error parsing line \", line)\n",
    "    if encode:\n",
    "        labels = [STR_TO_NUM[i] for i in labels]\n",
    "    return pd.DataFrame(data={\"text\": texts, \"labels\": labels})\n",
    "\n",
    "for filename in [train_full, train_dd, test_full, test_dd, dev_full, dev_dd]:\n",
    "    try:\n",
    "        _ = parse_fasttext_file(filename)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "\n",
    "def train_model(train_df, NUM_EPOCHS=30):\n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": NUM_EPOCHS,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"train_batch_size\": 32,\n",
    "        \"no_save\": True,\n",
    "        \"no_cache\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"save_steps\": -1,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"silent\": True\n",
    "    }\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        \"camembert\", \"EMBEDDIA/sloberta\",\n",
    "        num_labels = 21,\n",
    "        use_cuda = True,\n",
    "        args = model_args\n",
    "    )\n",
    "    model.train_model(train_df)\n",
    "    return model\n",
    "\n",
    "def eval_model(test_df):\n",
    "    y_true_enc = test_df.labels\n",
    "    y_pred_enc = model.predict(test_df.text.tolist())[0]\n",
    "\n",
    "    y_true = [NUM_TO_STR[i] for i in y_true_enc]\n",
    "    y_pred = [NUM_TO_STR[i] for i in y_pred_enc]\n",
    "\n",
    "    microF1 = f1_score(y_true, y_pred, labels=train_labels, average =\"micro\")\n",
    "    macroF1 = f1_score(y_true, y_pred, labels=train_labels, average =\"macro\")\n",
    "\n",
    "    return {\"microF1\": microF1, \n",
    "            \"macroF1\": macroF1}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = list()\n",
    "\n",
    "train_full_df = parse_fasttext_file(train_full)\n",
    "test_full_df = parse_fasttext_file(test_full)\n",
    "dev_full_df = parse_fasttext_file(dev_full)\n",
    "\n",
    "dev_dd_df = parse_fasttext_file(dev_dd)\n",
    "test_dd_df = parse_fasttext_file(test_dd)\n",
    "train_dd_df = parse_fasttext_file(train_dd)\n",
    "\n",
    "train_ok_df = parse_fasttext_file(train_ok)\n",
    "\n",
    "\n",
    "# First experiment: train on full, eval on all available \n",
    "for i in range(5):\n",
    "    print(\"Run \", i+1, \"of 5\")\n",
    "    model = train_model(train_full_df)\n",
    "\n",
    "    rundict = eval_model(test_full_df)\n",
    "    rundict[\"train\"] = \"full\"\n",
    "    rundict[\"eval\"] = \"test_full\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    rundict = eval_model(dev_full_df)\n",
    "    rundict[\"train\"] = \"full\"\n",
    "    rundict[\"eval\"] = \"dev_full\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    devtest_full_df = pd.concat([test_full_df, dev_full_df], ignore_index=True)\n",
    "\n",
    "    rundict = eval_model(devtest_full_df)\n",
    "    rundict[\"train\"] = \"full\"\n",
    "    rundict[\"eval\"] = \"devtest_full\"\n",
    "    results.append(rundict)\n",
    "\n",
    "\n",
    "# Second experiment: train on dedup, eval on all available \n",
    "for i in range(5):\n",
    "    print(\"Run \", i+1, \"of 5\")\n",
    "    model = train_model(train_dd_df)\n",
    "\n",
    "    rundict = eval_model(dev_dd_df)\n",
    "    rundict[\"train\"] = \"dd\"\n",
    "    rundict[\"eval\"] = \"dev_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    rundict = eval_model(test_dd_df)\n",
    "    rundict[\"train\"] = \"dd\"\n",
    "    rundict[\"eval\"] = \"test_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    devtest_dd_df = pd.concat([test_dd_df, dev_dd_df], ignore_index=True)\n",
    "\n",
    "    rundict = eval_model(devtest_dd_df)\n",
    "    rundict[\"train\"] = \"dd\"\n",
    "    rundict[\"eval\"] = \"devtest_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "# Third experiment: train on only keep, eval on all available \n",
    "for i in range(5):\n",
    "    print(\"Run \", i+1, \"of 5\")\n",
    "    model = train_model(train_ok_df)\n",
    "\n",
    "    rundict = eval_model(dev_dd_df)\n",
    "    rundict[\"train\"] = \"ok\"\n",
    "    rundict[\"eval\"] = \"dev_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    rundict = eval_model(test_dd_df)\n",
    "    rundict[\"train\"] = \"ok\"\n",
    "    rundict[\"eval\"] = \"test_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    devtest_dd_df = pd.concat([test_dd_df, dev_dd_df], ignore_index=True)\n",
    "\n",
    "    rundict = eval_model(devtest_dd_df)\n",
    "    rundict[\"train\"] = \"ok\"\n",
    "    rundict[\"eval\"] = \"devtest_dd\"\n",
    "    results.append(rundict)\n",
    "\n",
    "\n",
    "    rundict = eval_model(test_full_df)\n",
    "    rundict[\"train\"] = \"ok\"\n",
    "    rundict[\"eval\"] = \"test_full\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    rundict = eval_model(dev_full_df)\n",
    "    rundict[\"train\"] = \"ok\"\n",
    "    rundict[\"eval\"] = \"dev_full\"\n",
    "    results.append(rundict)\n",
    "\n",
    "    devtest_full_df = pd.concat([test_full_df, dev_full_df], ignore_index=True)\n",
    "\n",
    "    rundict = eval_model(devtest_full_df)\n",
    "    rundict[\"train\"] = \"ok\"\n",
    "    rundict[\"eval\"] = \"devtest_full\"\n",
    "    results.append(rundict)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'microF1': 0.585, 'macroF1': 0.571702071141847, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.575, 'macroF1': 0.5038823287921238, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.58, 'macroF1': 0.5404399893933838, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.585, 'macroF1': 0.5966322527043888, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.59, 'macroF1': 0.5424094544007411, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.5875, 'macroF1': 0.5722383837732403, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.625, 'macroF1': 0.622224565151736, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.555, 'macroF1': 0.5010989686144965, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.59, 'macroF1': 0.5567143173798803, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.6, 'macroF1': 0.5703267983378637, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.56, 'macroF1': 0.42015898713758115, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.58, 'macroF1': 0.5053397073132861, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.605, 'macroF1': 0.632368854106175, 'train': 'full', 'eval': 'test_full'}, {'microF1': 0.575, 'macroF1': 0.47780399931031653, 'train': 'full', 'eval': 'dev_full'}, {'microF1': 0.59, 'macroF1': 0.580195315365902, 'train': 'full', 'eval': 'devtest_full'}, {'microF1': 0.5879396984924623, 'macroF1': 0.5038281163281163, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.649746192893401, 'macroF1': 0.595564630743915, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6186868686868687, 'macroF1': 0.5803406826973889, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.5829145728643216, 'macroF1': 0.5207253296225728, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6040609137055838, 'macroF1': 0.5759216626719054, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.5934343434343434, 'macroF1': 0.5680276087477809, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.6130653266331658, 'macroF1': 0.5546765995553058, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6446700507614214, 'macroF1': 0.6244586400487021, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6287878787878788, 'macroF1': 0.6099193610826058, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.6080402010050251, 'macroF1': 0.562660449245815, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6345177664974619, 'macroF1': 0.5933186320037779, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6212121212121212, 'macroF1': 0.5904571590548051, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.6030150753768844, 'macroF1': 0.5716186201053716, 'train': 'dd', 'eval': 'dev_dd'}, {'microF1': 0.6345177664974619, 'macroF1': 0.6478570313974041, 'train': 'dd', 'eval': 'test_dd'}, {'microF1': 0.6186868686868687, 'macroF1': 0.6238302864618946, 'train': 'dd', 'eval': 'devtest_dd'}, {'microF1': 0.628140703517588, 'macroF1': 0.5517553731491014, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.5736040609137056, 'macroF1': 0.5375974918247926, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.601010101010101, 'macroF1': 0.5875220073937228, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.555, 'macroF1': 0.57934274719989, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.54, 'macroF1': 0.4589341610770183, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.5475, 'macroF1': 0.5499018167455739, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.6180904522613065, 'macroF1': 0.5902982296701889, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.583756345177665, 'macroF1': 0.5096477585435083, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.601010101010101, 'macroF1': 0.576492004981009, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.52, 'macroF1': 0.4530735897305683, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.545, 'macroF1': 0.5147501934458456, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.5325, 'macroF1': 0.5071389468406606, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.6030150753768844, 'macroF1': 0.5045843077140069, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.6142131979695431, 'macroF1': 0.5535002035002036, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.6085858585858586, 'macroF1': 0.5527047584578121, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.505, 'macroF1': 0.4668973561223078, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.515, 'macroF1': 0.37032360670609077, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.51, 'macroF1': 0.44532519958269345, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.5879396984924623, 'macroF1': 0.5431382061183072, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.6142131979695431, 'macroF1': 0.5419233676376534, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.601010101010101, 'macroF1': 0.5814259630191666, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.565, 'macroF1': 0.5492759431653833, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.52, 'macroF1': 0.44482366817028324, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.5425, 'macroF1': 0.5204857435722245, 'train': 'ok', 'eval': 'devtest_full'}, {'microF1': 0.6030150753768844, 'macroF1': 0.5288481853331477, 'train': 'ok', 'eval': 'dev_dd'}, {'microF1': 0.5939086294416244, 'macroF1': 0.5657871725845822, 'train': 'ok', 'eval': 'test_dd'}, {'microF1': 0.5984848484848485, 'macroF1': 0.5797093299185282, 'train': 'ok', 'eval': 'devtest_dd'}, {'microF1': 0.555, 'macroF1': 0.5836125025162565, 'train': 'ok', 'eval': 'test_full'}, {'microF1': 0.545, 'macroF1': 0.44747874696681805, 'train': 'ok', 'eval': 'dev_full'}, {'microF1': 0.55, 'macroF1': 0.5410152446123593, 'train': 'ok', 'eval': 'devtest_full'}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"backup_17.txt\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>microF1</th>\n",
       "      <th>macroF1</th>\n",
       "      <th>train</th>\n",
       "      <th>eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.594</td>\n",
       "      <td>0.566</td>\n",
       "      <td>ok</td>\n",
       "      <td>test_dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.598</td>\n",
       "      <td>0.580</td>\n",
       "      <td>ok</td>\n",
       "      <td>devtest_dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.555</td>\n",
       "      <td>0.584</td>\n",
       "      <td>ok</td>\n",
       "      <td>test_full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.447</td>\n",
       "      <td>ok</td>\n",
       "      <td>dev_full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.541</td>\n",
       "      <td>ok</td>\n",
       "      <td>devtest_full</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    microF1  macroF1 train          eval\n",
       "55    0.594    0.566    ok       test_dd\n",
       "56    0.598    0.580    ok    devtest_dd\n",
       "57    0.555    0.584    ok     test_full\n",
       "58    0.545    0.447    ok      dev_full\n",
       "59    0.550    0.541    ok  devtest_full"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option(\"precision\", 3)\n",
    "with open(\"backup_17.txt\") as f:\n",
    "    content = json.load(f)\n",
    "jsonlikecontent = dict()\n",
    "for key in content[0].keys():\n",
    "    jsonlikecontent[key] = [i[key] for i in content]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=jsonlikecontent)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">microF1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">macroF1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <th>eval</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">dd</th>\n",
       "      <th>dev_dd</th>\n",
       "      <td>0.599</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devtest_dd</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_dd</th>\n",
       "      <td>0.634</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">full</th>\n",
       "      <th>dev_full</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devtest_full</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_full</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">ok</th>\n",
       "      <th>dev_dd</th>\n",
       "      <td>0.608</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev_full</th>\n",
       "      <td>0.533</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devtest_dd</th>\n",
       "      <td>0.602</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devtest_full</th>\n",
       "      <td>0.536</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_dd</th>\n",
       "      <td>0.596</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_full</th>\n",
       "      <td>0.540</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   microF1        macroF1       \n",
       "                      mean    std    mean    std\n",
       "train eval                                      \n",
       "dd    dev_dd         0.599  0.013   0.543  0.029\n",
       "      devtest_dd     0.616  0.013   0.595  0.022\n",
       "      test_dd        0.634  0.018   0.607  0.029\n",
       "full  dev_full       0.571  0.014   0.489  0.045\n",
       "      devtest_full   0.585  0.005   0.551  0.030\n",
       "      test_full      0.600  0.017   0.599  0.028\n",
       "ok    dev_dd         0.608  0.015   0.544  0.032\n",
       "      dev_full       0.533  0.014   0.447  0.052\n",
       "      devtest_dd     0.602  0.004   0.576  0.013\n",
       "      devtest_full   0.536  0.016   0.513  0.041\n",
       "      test_dd        0.596  0.018   0.542  0.021\n",
       "      test_full      0.540  0.026   0.526  0.062"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=[\"train\", \"eval\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                          |   ('microF1', 'mean') |   ('microF1', 'std') |   ('macroF1', 'mean') |   ('macroF1', 'std') |\n",
      "|:-------------------------|----------------------:|---------------------:|----------------------:|---------------------:|\n",
      "| ('dd', 'dev_dd')         |              0.598995 |           0.0130072  |              0.542702 |            0.0290347 |\n",
      "| ('dd', 'devtest_dd')     |              0.616162 |           0.0133624  |              0.594515 |            0.0224479 |\n",
      "| ('dd', 'test_dd')        |              0.633503 |           0.0177302  |              0.607424 |            0.0285351 |\n",
      "| ('full', 'dev_full')     |              0.571    |           0.0138744  |              0.489071 |            0.0449579 |\n",
      "| ('full', 'devtest_full') |              0.5855   |           0.00512348 |              0.550986 |            0.0297173 |\n",
      "| ('full', 'test_full')    |              0.6      |           0.0165831  |              0.598651 |            0.0283954 |\n",
      "| ('ok', 'dev_dd')         |              0.60804  |           0.0154885  |              0.543725 |            0.0315722 |\n",
      "| ('ok', 'dev_full')       |              0.533    |           0.0144049  |              0.447262 |            0.051518  |\n",
      "| ('ok', 'devtest_dd')     |              0.60202  |           0.00382974 |              0.575571 |            0.0133973 |\n",
      "| ('ok', 'devtest_full')   |              0.5365   |           0.0162596  |              0.512773 |            0.0412814 |\n",
      "| ('ok', 'test_dd')        |              0.595939 |           0.018161   |              0.541691 |            0.020998  |\n",
      "| ('ok', 'test_full')      |              0.54     |           0.0259808  |              0.52644  |            0.0622833 |\n"
     ]
    }
   ],
   "source": [
    "print(_.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
